---
title: '백엔드 개발자를 위한 한 번에 끝내는 대용량 데이터 & 트래픽 처리 초격차 패키지 - 대용량 처리를 위한 MySQL 이해'

categories:
  - Lecture
tags:
  - []

toc: true
toc_sticky: true

date: 2023-01-24
last_modified_at: 2023-01-29
---

# Part 1. 대용량 처리를 위한 MySQL 이해

## [Ch 02. 대용량 시스템에 대한 이해]

- 웹의 기본 아키텍처는 "웹서버, 데이터베이스" 2 Tier 구조였으나, 트래픽의 증가로 인해 "웹 어플리케이션 서버, 웹 서버, 데이터베이스" 3 Tier 구조로 변경되었다.
- 웹서버들의 경우 스케일 아웃을 통해 병목을 줄일 수 있으나, 데이터베이스의 경우 "데이터" 라는 상태를 관리하고 있어 스케일 아웃을 하는데 많은 비용이 소모된다.
- 또한 서버의 경우 메모리의 데이터를 이용하지만, 데이터베이스의 경우 디스크의 데이터에 접근하여 처리를 하기 때문에 느릴 수 밖에 없다.

- 마이크로 서비스 아키텍처 분산 환경에서는 대용량 시스템이 어려운 이유

  - 하나의 서버로 감당하기 힘들어 여러개의 서버 또는 데이터베이스를 사용
  - 데이터의 일관성을 보장할 수 있어야 함
  - 서비스들이 얽혀있어 시스템 복잡성이 증가함

- 대용량 시스템을 위해 갖춰야할 부분
  - 고가용성
  - 확장성
  - 관측가능성

![마이크로 서비스 아키텍처 기반 대용량 시스템 샘플](/assets/images/learn/lecture/2023-01-24%20MSA.png)

- 데이터베이스의 병목 현상을 줄이기 위해 "캐시 서버" 를 만들 수 있다.
- 클라이언트의 요청 후 대기 시간을 "비동기 큐" 를 이용하여 줄일 수 있다.
- 이 외에도 마이크로 서비스들을 관리하기 위한 여러 시스템이 추가될 수 있다.

## [Ch 03. MySQL 아키텍처 소개]

![MySQL 아키텍처](/assets/images/learn/lecture/2023-01-24%20MySQL%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98.png)

- 참고문헌 [2] 내용 확인

## [Ch 04. SNS 모델링으로 배우는 정규화/비정규화]

- 핵심은 중복을 최소화하는 행위

- 대용량 시스템을 설계하는 관점에서 조회와 쓰기를 다르게 보아야한다!

- 정규화
  - 중복을 제거하고 한 곳에서 관리
  - 데이터 정합성 유지가 쉬움
  - 읽기시 참조 발생
- 반정규화
  - 중복을 허용
  - 데이터 정합성 유지가 어려움
  - 참조없이 읽기 가능

### 실무에서의 고민

- 정규화 과정에도 비용이 들기 때문에 고려해야할 사항이 많다.
  - 얼마나 빠르게 데이터의 최신성을 보장해야 하는가?
  - 히스토리성 데이터는 오히려 정규화를 하지 않아야 한다.
  - 데이터 변경 주기와 조회 주기는 어떻게 되는가?
  - 객체(테이블) 탐색 깊이가 얼마나 깊은가?
    - 객체의 탐색 깊이가 깊어질때 찾고자 하는 깊이의 데이터를 앞으로 가져오는 것!
    - 하지만 이 경우 데이터 정합성이 유지되도록 처리가 필요
- 정규화를 하기로 했다면 읽기시 데이터를 어떻게 가져올 것인가?
  - 테이블 조인보다는 다른 방법을 고민!
  - 테이블 조인은 서로 다른 테이블의 결합도를 엄청나게 높인다.
  - 조회시에는 성능이 좋은 별도 데이터베이스나 캐싱 등 다양한 최적화 기법을 사용할 수 있다.
  - 조인을 사용하게 되면, 이런 기법들을 사용하는데 제한이 있거나 더 많은 리소스가 들 수 있다.
  - 읽기 쿼리 한번 더 발생되는 것은 그렇게 큰 부담이 아닐 수 있다.

## [Ch 05. 조회 최적화를 위한 인덱스 이해하기]

### 데이터베이스 성능 핵심

- 데이터를 저장하기 위한 용도로 "메모리, 디스크" 를 가장 많이 사용한다.
- 디스크는 메모리에 비해 속도가 느리다.
  - 그렇기 때문에 데이터베이스 성능의 핵심은 디스크 I/O 를 최소화 하는 것
  - 메모리에 올라온 데이터로 최대한 요청을 처리하는 것 (메모리 캐시 히트율을 높이는 것)
- 메모리 데이터 유실을 고려하여 WAL (Write Ahead Log)를 사용
  - 대부분의 트랜잭션은 무작위하게 Write 를 발생
  - 이를 지연시켜 랜덤 I/O 횟수를 줄이는 대신 순차적 I/O 를 발생시켜서 정합성 유지

### 인덱스의 기본 동작

- 인덱스는 정렬된 자료 구조, 이를 통해 탐색 범위를 최소화
- 인덱스의 핵심은 탐색 범위를 최소화 하는 것

### 인덱스 자료구조

- B+ Tree
- 삽입, 삭제시 계속 트리 리밸런싱이 이루어짐
- 트리의 깊이를 조절할 수 있어서 탐색하는데 유리 (탐색 시간은 트리의 깊이에 비례)

### 클러스터 인덱스

- 클러스터 인덱스는 데이터 위치를 결정하는 키 값이다.
- MySQL의 PK 는 클러스터 인덱스다.
- MySQL에서 PK 를 제외한 모든 인덱스는 PK 를 가지고 있다.

- PK 순서에 따라서 데이터 저장 위치가 달라진다.

  - PK 키 삽입/갱신시에 성능이슈 발생
  - PK 로 Auto Increments vs UUID 찾아보기
    - UUID 의 경우 16byte, Auto Increments 의 경우 Long 타입으로 8byte 이기 때문에 더 적은 공간을 사용한다.
    - 더 많은 공간을 사용하는 경우 메모리, 디스크 사용량이 늘어남에 따라 성능 저하가 발생한다.
    - 하지만 동시성 작업이 많은 로깅 시스템, 대규모 시스템 등의 경우에는 Auto Increments 의 경우 중복이 발생할 수 있기 때문에 UUID 를 고려해볼만하다.

- 클러스터 인덱스 장점
  - PK를 활용한 검색이 빠름, 특히 범위 검색!
  - 세컨더리 인덱스들이 PK를 가지고 있어 커버링에 유리

### 성능 테스트를 위한 계시물 벌크 인서트 구현

- Bulk Insert 기능!!

  - Spring Data JPA를 이용하는 경우 save()를 루프 돌면서 호출
  - 그래서 PK 가 auto increment 인 경우 JdbcTemplate 을 자주 사용!

- 다량의 데이터를 한번에 삽입시 많은 Memory를 사용하기 때문에 OOM(Out of Memory) 이슈가 발생할 수 있음

### 인덱스 추가 후 성능 비교

- 데이터의 분포에 따라 동일한 인덱스여도 조회 속도 차이가 발생할 수 있다.

  - 그렇다면 어떤 데이터가 많이 생기는지? 에 대한 분석도 필요!
  - 데이터 분포에 대한 분석이 선행되어야 어떤 인덱스를 추가하고, 쿼리를 수정할지 결정

- explan 키워드를 통해 인덱스가 정상적으로 적용되었는지 여부도 항상 확인하는 습관 만들기!!

### 인덱스를 다룰 때 주의해야 할 점

- 인덱스 구조는 B-Tree 구조이기 때문에 인덱스 필드에 가공이 생겼을시 인덱스 필드를 탈 수 없다.

  - 인덱스에 연산 과정을 거친다던가
  - 인덱스 타입이 변경된다던가

- 복합 인덱스의 경우 A, B ... 필드 순으로 정렬된다.

  - 과일 인덱스 / 과일, 원산지 인덱스 그림을 그려보기!

- 하나의 쿼리에는 하나의 인덱스만 탄다.
- 여러 인덱스 테이블을 동시에 탐색하지 않음

  - index merge hint 를 사용하면 가능은 함
  - 그러므로 where, order by, group by 를 혼합하여 사용하는 경우 인덱스를 잘 고려해야함!

- 의도대로 인덱스가 동작하지 않을 수 있기 때문에 'explan' 으로 쿼리 통계를 확인해야 함

  - local, dev, prod 환경별로 다를 수 있기 때문에 전체 확인 필요

- 인덱스도 비용을 소모하기 때문에 쓰기를 희생하고 조회를 얻는 것!
- 꼭 인덱스로만 해결할 수 있는 문제인지 생각해봐야함!

  - 조회 조건은 생각보다 자주 바뀌기 때문에 인덱스로 해결할 수 없는 순간도 발생
  - 그렇다면 별도의 데이터베이스를 사용하다던가 / 읽기, 쓰기 모델을 분리한다던가 / 캐싱을 추가한다던가 ...
  - 이런 고민하는 습관이 성장하는데 매우 중요!

- 인덱스 선정시, 카디널리티! 즉 식별할 수 있는 양이 많은 것을 고르자
  - 성별을 인덱스로 하는 경우 남/녀 2가지로만 가능하기 때문에 별로 좋지 않음
  - 이름 등과 같이 여러 케이스가 있는 것을 사용하는 것을 추천

## [Ch 06. 페이지네이션 최적화]

### 페이지네이션이란

- 많은 양의 데이터를 효율적으로 페이지 형태로 제공
- 오프셋 기반, 커서 기반 등이 있음

### 오프셋 기반 페이징

- 계시판 성격에 맞는 방법
- offset, size 를 같이 입력 받아서 특정 데이터의 시작점으로부터의 갯수를 반환
- 전체 데이터의 갯수를 알아야하는 문제가 있음!
  - 그렇기에 대용량 데이터를 다룰 때는 마지막 페이지를 표기하지 않거나 다른 페이징 기법을 사용하는 경우가 많음

### 커서 기반 페이징

- 스크롤 형태의 계시글 성격에 맞는 방법
- 하지만 마지막 페이지 여부를 알아야 더 이상 스크롤을 하지 않기 때문에 마지막 페이지 여부를 인식하기 위한 필드는 넣어주는게 좋음

### 커버링 인덱스

- 테이블에 접근하지 않고 인덱스로만 데이터 응답이 가능하다면, 인덱스로만 데이터를 제공하겠다.
- 테이블에 직접 접근하지 않기 때문에 매우 빠르다!
- 커버링 인덱스를 이용하면 불필요한 데이터 접근을 최소화할 수 있다.
  - order by, offset, limit 절로 인한 불필요한 데이터블록 접근을 커버링 인덱스를 통해 최소화

```SQL
with 커버링 as (
  SELECT id
  FROM 회원
  WHERE 나이 < 30
  LIMIT 2
)

SELECT 이름
FROM 회원 INNER JOIN 커버링 ON 회원.id = 커버링.id
```

## [Ch 07. 타임라인 최적화]

### 타임라인이란?

- 트위터, 페이스북, 인스타그램 등 SNS에서 팔로워들의 계시물을 보여주는 피드

### 팬아웃 타임라인

1. Fan Out On Read (Pull Model)

- 시간 복잡도 = log(Follow 전체 레코드) + 해당 회원의 Following \* log(Post 전체 레코드)
- 오래 사용한 사용자일 수 록 부하가 발생 ...
- 사용자가 매번 홈에 접속할 때마다 부하가 발생
- Key Point!!! 시간 복잡도를 구할 수 있고, 시간 복잡도의 병목 구간을 캐치할 수 있으면 좋겠다!

2. Fan Out On Write (Push Model)

- 계시물 작성시, 해당 회원을 팔로우하는 회원들에게 데이터를 배달한다.
- 타임라인 조회시에는 Timeline 테이블을 조회하여 계시물들 조회!
- Pull 모델에서의 조회시점 부하를 쓰기시점의 부하로 변경

### 타임라인에서 배우는 트레이드 오프

- Push Model은 공간복잡도를 희생, Pull Model은 시간 복잡도를 희생
  - Push Model에서는 계시물 작성과 타임라인 배달의 정합성 보장에 대한 고민이 필요하다.
  - 비동기 또는 다른 저장소 사용 ...
  - ["CAP 이론"](https://dongwooklee96.github.io/post/2021/03/26/cap-%EC%9D%B4%EB%A1%A0%EC%9D%B4%EB%9E%80/)
  - Push Model은 Pull Model에 비해 시스템 복잡도가 높지만 그만큼 비즈니스, 기술 측면에서 유연성을 확보시켜 준다.
- 은총알은 없다! 상황, 자원, 정책 등 여러가지를 고려해 최선의 트레이드 오프를 해야한다.

## [Ch 08. 데이터 정삽성 보장을 위한 트랜잭션 이해하기]

### 트랜잭션 ACID

- 트랜잭션이 진행 중인 대상을 조회함으로써 데이터 정합성이 깨지거나, 트랜잭션이 실패하여 데이터 정삽성이 깨지거나 할 수 있다.
- ACID 개념 (개념뿐 아니라 기술의 등장 배경과 원리가 중요)
  - A: 원자적 연산을 보장해야 한다. ALL or Nothing
    - MVCC를 통해 보장
    - Inno DB에 있는 원본 데이터를 Undo Log에 보관함으로써 트랜잭션 실패시 롤백 처리
    - 트래잭션이 Atomicity한 단위가 된다.
  - C: 트랜잭션이 종료되었을 때 무결성이 보장된다.
    - 제약조건을 통해 보장
  - I: 트랜잭션은 서로 간섭하지 않고 독립적으로 동작
    - 많은 성능을 포기해야하므로 개발자가 제어 가능
    - 트랜잭션 격리 레벨을 통해 via MVCC
  - D: 완료된 트랜잭션을 유실되지 않아야 한다.
    - WAL(Write-Ahead Logging)을 통해 로그를 남기게 되면 완전히 DB에 들어가지 못한 데이터를 재처리 할 수 있다.

### 트랜잭션 격리레벨

- ISOLATION = 트랜잭션은 서로 간섭하지 않고 독립적으로 보장한다.
  - 종류: READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE READ
  - 이상 현상: Dirty Read, Non Repeatable Read, Phantom READ
- Dirty Read = 커밋되지 않은 데이터를 읽었다.
  - 트랜잭션 롤백을 위해 Undo Log에 저장된 데이터를 읽는다.
- Non Repeatable Read = 같은 데이터를 조회하였는데 결과가 달라지는 현상
- Phantom READ = 같은 조건으로 데이터를 읽었을 때 없던 데이터가 생기는 현상 (update, insert, delete ...)

- Key Point! 격리 레벨별 어떤 이상 현상이 발생할 수 있는지를 정리
- 아래로 갈 수록 이상 현상이 적어지지만, 동시 처리량이 적어진다.
  - 그래서 주로 READ COMMITTED를 사용
  - [Lost Update, Read Skew, Write Skew 학습](https://vladmihalcea.com/write-skew-2pl-mvcc/)

# 참고문헌

[1] 백엔드 개발자를 위한 한 번에 끝내는 대용량 데이터 & 트래픽 처리, https://fastcampus.co.kr/dev_online_bedata<br>
[2] Architecture of MySQL, https://www.geeksforgeeks.org/architecture-of-mysql/<br>
[3] CAP 이론이란?, https://dongwooklee96.github.io/post/2021/03/26/cap-%EC%9D%B4%EB%A1%A0%EC%9D%B4%EB%9E%80/<br>
[4] 트랜잭션, https://velog.io/@jin942002/%ED%8A%B8%EB%9E%9C%EC%9E%AD%EC%85%98
